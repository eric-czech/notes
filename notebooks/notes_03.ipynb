{
 "metadata": {
  "name": "",
  "signature": "sha256:017dca8cde7a590260966bf0d4b15a29c27dd6eba9af316977c029a7e0cd45a6"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%load_ext rpy2.ipython"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "References:\n",
      "- AML = Applied Machine Learning (Max Kuhn)\n",
      "- ESL = Elements of Statistical Learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Generating Random Samples</center>\n",
      "\n",
      "[Source](http://stats.stackexchange.com/questions/120179/generating-data-with-a-given-sample-covariance-matrix)\n",
      "- To genereate data with sample mean $\\mu$ and covariance $\\Sigma$, generate $z$ as standard normal (iid) variables and compute sample vector $y$ as $y = Lz + \\mu$ where $\\mu$ is desired means and $L$ is the left matrix of a Cholesky decomposition of $\\Sigma$\n",
      "- To generate data with **precisely** mean and covariance $\\mu$ and $\\Sigma$, you do the same as the above except you standardize (subtract mean, divide by SD) the iid sample vector $z$ before multiplying by Cholesky factor"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Support Vector Machines</center>\n",
      "\n",
      "- [Source](https://www.youtube.com/watch?v=_PwhiWxHK8o)\n",
      "\n",
      "- Decision function\n",
      "    - Objective is to determine vector $\\hat{w}$ that is orthogonal to separating hyperplane\n",
      "    - The length of the projection of a new sample onto $\\hat{w}$ is given by the dot product of the new sample and normalized $\\hat{w}$\n",
      "    - If the above projection goes beyond hyperplane, it is a positive sample or vise-versa\n",
      "- Fitting\n",
      "    - Objective is to maximize \"gutters\" around hyperplance\n",
      "    - Width of gutter is equal to projection of difference between a positive point (as vector) and negative point\n",
      "    - The above expression ultimately leads to an objective function to minimize as simply $\\Vert \\hat{w} \\Vert$\n",
      "    - The constraints for that optimization are given as inequality conditions which then means that the objective functions and constraints can be written as a single function, using Lagrange Multipliers:\n",
      "    $$ F = \\frac{1}{2} \\Vert \\hat{w} \\Vert ^2 - \\sum{\\alpha_i [ y_i ( \\hat{w} \\cdot \\hat{x}_i + b) - 1 ] },$$\n",
      "    $$\\text{where }y_i = 1 \\text{ for positive case }x_i\\text{ or -1 for negative case}$$\n",
      "        - Note that Lagrange Multipliers are usually used in this way for constrained optimization with **equality** conditions; however, the [KKT](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions) conditions extend this idea and allow for the same technique to be applied with inequality constraints\n",
      "    - If you set derivatives above with respect to $\\hat{w}$ and $b$ to 0 and solve, you ultimately end up with an expression for the maximized $F$ that only uses dot products of **pairs** of $x_i$ samples (this is why the kernel trick helps)\n",
      "- Weights for Optimization \n",
      "    - In the above optimization, you can see that the objection function to optimize would allow for $\\hat{w}$ and $b$ to change.  Note that $\\hat{w}$ is like the coefficients applied in linear regression, one for each feature.  This becomes prohibitive though once the feature vectors, $x_i$, are mapped to a high dimensional space (i.e. too many free variables to optimize over).  This is overcome then by the [Representer Theorem](https://en.wikipedia.org/wiki/Representer_theorem) which allows for $\\hat{w}$ to be represented by $\\sum_i^n{ \\alpha_i \\phi(x_i) }$ instead (where $\\phi$ is the basis function).  This means that the decision function can then be written as $\\sum_i^n { \\alpha_i \\phi(x_i) \\phi(x) } = \\sum_i^n { \\alpha_i K(x_i, x) }$.  Therefore the optimization is over a weight per observation (ie $\\alpha$) instead of per feature.\n",
      "    - [Source for above](http://www.cs.columbia.edu/~kathy/cs4701/documents/jason_svm_tutorial.pdf) (slide 24)\n",
      "- [Kernel Trick](http://www.cs.berkeley.edu/~jordan/courses/281B-spring04/lectures/lec3.pdf)\n",
      "    - The point of using Kernels is to solve a linear problem for original features mapped to higher dimensional space\n",
      "    - The functions that map features from a single observation to that higher dimensional space are basis functions\n",
      "    - One such basis could be $(x_1, x_2) -> (z_1, z_2, z_3) = (x_1^2, \\sqrt{2}x_1x_2, x_2^2)$\n",
      "    - In this case, since kernel learning methods only require the kernel matrix and in this case the kernel used by SVM's is always the dot product of observations, the only thing necessary for optimization is the dot product of each point after running it through the basis function.\n",
      "    - For example, say $r$ and $s$ are two (3-dimensional) vectors already run through the basis function using $a$ and $b$ as inputs (2-dimensional).  Now, the dot product of these two is $\\langle r, s \\rangle = r_1s_1 + r_2s_2 + r_3s_3 = a_1^2b_1^2 + 2 a_1a_2b_1b_2 + a_2^2b_2^2 = (a_1b_1 + a_2b_2)^2 = \\langle a, b \\rangle ^2$.\n",
      "    - In the above the \"Kernel Trick\" is simply the idea that the dot product in the higher dimensional space can be computed as a simple function of the dot product in the original space (in this case that dot product is just squared).  This way, the basis functions **never actually have to be used**, they can simply be bypassed since the optimization problem only requires dot products between points.\n",
      "    \n",
      "### Relationship to KNN\n",
      "\n",
      "- Kernel methods are similar to kNN, but would be more so if kNN worked by picking the size of a neighborhood in distance units rather than in the number of nearest members.\n",
      "- Parzen Window Classifiers work even more similarly to SVM's by fixing the size of the neighborhood, though the \"kernel\" used here would still be a step function\n",
      "\n",
      "### Kernelized KNN\n",
      "\n",
      "- [Source](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.125.3253&rep=rep1&type=pdf)\n",
      "- Same as standard KNN but calculates distance between points as $K(x, x) - 2K(x, y) + K(y, y)$ where $K$ is the chosen kernel.  Note that RBF kernels and Polynomial kernel with degree 1 degenerate to standard KNN algorithm\n",
      "\n",
      "### RBF Kernel\n",
      "\n",
      "- While the feature map for polynomial kernels is simpler (and of finite dimension), the feature map associated with the RBF kernel is infinitely dimensional (see [this post](https://www.quora.com/Why-does-the-RBF-radial-basis-function-kernel-map-into-infinite-dimensional-space))\n",
      "- Note that the distance in the RBF kernel spec, $e^{\\frac{-2||x - y||^2}{2\\sigma^2}}$ in the RBF kernel can be converted algebraically to $e^{\\frac{\\langle x, y \\rangle}{\\sigma^2}}$ (also in post above).  This means that RBF kernel like all others, is just a function of the dot product of two observations in original feature space."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Boosting</center>\n",
      "\n",
      "### AdaBoost\n",
      "\n",
      "- Formulated by Yoav Freund and Robert Schapire in 1999 who won the G\u00f6del Prize in 2003 for their work\n",
      "- Algorithm:\n",
      "\n",
      "    For $K$ stages, with $n$ samples:\n",
      "    - Initialize weights of each sample as $1/n$\n",
      "    - For $k=1$ to $K$\n",
      "        - Fit weak learner ($h_k$) to weighted samples\n",
      "        - Compute misclassification error as $e_k$\n",
      "        - Compute stage weight as $\\alpha_k = ln(\\frac{(1 - e_k)}{e_k})$\n",
      "        - Update sample weights so that misclassified samples have higher weight ($w_{k,i} = w_{k-1,i} e^{-y_i \\alpha_k h_k(x_i)}$)\n",
      "    - Make predictions as sum of stage weights times predictions for weak learnings at each stage\n",
      "    \n",
      "### Gradient Boosting\n",
      "\n",
      "- First noted by Leo Brieman in 1997, explicitly proposed by Jerome Friedman in 1999\n",
      "- Algorithm:\n",
      "\n",
      "    For $K$ iterations, with $n$ samples, and differentiable loss function $L$:\n",
      "    - Initialize predictions for classifier as probability across whole sample (ie $F_0(x) = n_1 / n$)\n",
      "    - For $k=1$ to $K$\n",
      "        - Compute residual gradient outcome for each $r_i, i \\in [1,n]$ as the partial derivative of the loss function with respect to the predicted ensemble value $F(x_i)$\n",
      "        - Fit learner $h_k(x)$ to residuals computed above\n",
      "        - Find best $\\gamma$ such that $L(y_i, F_{k-1} + \\gamma h_k(x))$ is at a minimum\n",
      "        - Set $F_k = F_{k-1} + \\nu \\gamma h_k(x)$ where $\\nu$ is the \"shrinkage\" parameter (usually around .1)\n",
      "    - Output $F_K$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Decision Trees</center>\n",
      "\n",
      "- Definition of Gini Index (for binary class) for a single node:  $p_1(1 - p_1) + p_2(1 - p_2) = 2p_1p_2$ where $p_1$ is number of samples with class 1 divided by number of samples; $p_2$ is the same thing but for class 2 (note that $1 - p_1 = p_2$ by that definition)\n",
      "- Value of Gini Index at a split = value of gini index for each node on side of split weighted by the number of samples on either side (i.e. weight for left side = $\\frac{n_{left}}{n} \\cdot gini(left)$)\n",
      "- Handling of categorical variables:\n",
      "    1. Create dummy variables allowing a split to be the presence of that category or not\n",
      "    2. Allowing multiple categories to be grouped together in a single split (this implies an order of the categories and sometimes leads to situations where only one category really matters but others are included anyhow)\n",
      "- **Selection Bias** in the context of trees refers to how predictors with more possible values are more likely to be chosen as splits; conditional inference trees can be used instead which use t-tests and the like with multiple hypothesis test adjustments to get unbiased estimates of splits\n",
      "- **$c_p$ parameter** - For vanilla trees, $SSE_{c_p} = SSE + c_p \\cdot \\text{# Terminal Nodes}$\n",
      "- Single trees are **unstable** (Hastie and Breiman) -- If the data are slightly altered, a completely different set of splits might be found"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Profile Likelihood</center>\n",
      "\n",
      "[Source](http://people.upei.ca/hstryhn/stryhn208.pdf)\n",
      "\n",
      "As a start, compute the ML estimates ($\\theta^*$,$\\delta^*$) and the corresponding log-likelihood value. Then proceed by the following steps:\n",
      "1. Compute a \u201creasonable\u201d lower bound $\\theta_L$ for the lower confidence limit (e.g.,\n",
      "$\\theta^*$ - 5 SE($\\theta^*$), or 0.0001 if $\\theta$-values are restricted to be >0)\n",
      "2. Compute uppder bound $\\theta_H$\n",
      "2. Define a grid of values ranging from $\\theta_L$ to $theta_H$ (e.g., 100 equidistant points)\n",
      "3. For each grid value $\\theta_i$, compute the profile log-likelihood value $ln(L(\\theta_i))$ by maximizing\n",
      "the $ln(L(\\theta_i, \\delta))$ over $\\delta$-values (a standard analysis allowing $\\theta$ to be fixed at\n",
      "$\\theta_i$ may apply; for example, run lm with fixed offset if profiling for intercept [i.e. maximimize over slope coefficient])\n",
      "4. Take as the lower bound ($\\theta_{CI_L}$) of the 95% CI the smallest $\\theta_i$-value for which it\n",
      "holds that $ln(\\frac{L(\\theta^*, \\delta^*)}{L(\\theta_i)}) \\leq T$ where $T$ is critical chi square value for desired level of significance\n",
      "5. If necessary, refine or extend the grid of values around $\\theta_{CI_L}$ to obtain greater accuracy."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Feature Selection</center>\n",
      "\n",
      "### With Caret\n",
      "\n",
      "Example:\n",
      "\n",
      "```\n",
      "mySBF <- lmSBF\n",
      "mySBF$filter <- function(score, x, y) { score <= 10e-6 }\n",
      "fit <- sbf(form = Class ~ .,\n",
      "           data = simdata, \n",
      "           method = \"svmLinear\",\n",
      "           trControl = trainControl(method = \"cv\", number = 2, classProbs = TRUE),\n",
      "           tuneGrid = data.frame(C = c(0.25, 0.5)),\n",
      "           preProc = c(\"center\", \"scale\"),\n",
      "           sbfControl = sbfControl(\n",
      "               functions = mySBF, method = 'repeatedcv',\n",
      "               number = 4, repeats = 10)\n",
      "       )\n",
      "```\n",
      "\n",
      "In this case, the ```sbfControl``` settings take care of the outer cross validation where for each fold the feature selection routine is applied.  The given estimator is then trained on each of those folds possibly with another level of CV to train over hyperparameters.  Note that in the case of one hyperparameter (i.e. a static model), the **method** arg in ```trainControl``` can be set to 'none'.  The same is not true for ```sbfControl``` however as the outer CV is necessary.\n",
      "\n",
      "## A few methods for feature selection\n",
      "- Wrapper Methods \n",
      "    - Forward selection\n",
      "        - (AML p 491) Start with intercept, create P models for P predictors\n",
      "        - Update overall best model with single predictor having lowest p-value (or giving best possible score for model (e.g. RMSE))\n",
      "        - Repeat until no new predictors are statistically significant or until some other criteria\n",
      "    - Backward selection - same as above in reverse\n",
      "    - RFE\n",
      "        - Select model M (can be RF or anything with variable importances)\n",
      "        - Score all features using model\n",
      "        - For $i$ = 1 .. $p$ (where $p$ is number of features), determine score (possibly CV score) for model using top $i$ features\n",
      "        - Use model with maximum score and best $i$ features\n",
      "- Filter Methods\n",
      "    - \n",
      "\n",
      "## Selection Bias\n",
      "\n",
      "- (AML p 500) Guyon et al. 2002 attempted to show how RFE used with SVM could achieve 100% accuracy on colon cancer data\n",
      "    - Was mistaken and had applied RFE outside of resampling\n",
      "- Proper resampling includes applying feature selection in outer CV loop with parameter tuning occurring within inner CV loops (AML p 501)\n",
      "- Always use a testing set when p >> n to avoid potential selection bias\n",
      "- The lack of increased performance in ensembling may indicate measurement error in predictors (AML p 527)\n",
      "- A way to remember intuition for selection bias:\n",
      "    - Imagine how an external feature selector would **remove** the features that would typically make a classifier worse in cross validation if the whole feature set was used\n",
      "\n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>High Dimensional Problems</center>\n",
      "\n",
      "*The following taken from ESL chapter 18*\n",
      "- Notes on ridge regression in high dimensions (ESL p. 650):\n",
      "    - In a simulation, $N$ = 100 samples were used with $p$ = 20, 100, and 1000 with $y$ created from a multivariate $X$ of dimension $p$ where each feature had a pairwise correlation of .2\n",
      "    - Ridge regression models were run\n",
      "    - With $p < N$, the number of nonzero coefficients equals number expected \n",
      "    - With $p >> N$ however, the number of nonzero coefficients was unexpectedly small and the average number of times the absolute value for coefficients were greater than 2 for each case of p = 20, 100, and 1000 was 9.8, 1.2, and 0 respectively\n",
      "    - **Conclusion** When $p >> N$ it is not uncommon for recovery of actual coefficients to become impossible\n",
      "- See the code below for an example showing a similar effect with LASSO regression:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%%R\n",
      "library(MASS)\n",
      "library(glmnet)\n",
      "library(foreach)\n",
      "library(ggplot2)\n",
      "\n",
      "set.seed(1)\n",
      "N <- 100\n",
      "r <- foreach(p=c(seq(10, 500, length.out=20), seq(250, 2000, by=250)), .combine=rbind)%do%{\n",
      "  q <- ceiling(p * .1) # Choose number of features that matter\n",
      "  X <- mvrnorm(n=N, rep(0, p), diag(rep(.8, p)) + .2)\n",
      "  b <- c(rnorm(q), rep(0, p - q))\n",
      "  y <- X %*% b + rnorm(n=N, sd = 1) \n",
      "  m <- cv.glmnet(x = X, y = y, alpha = 1)\n",
      "  \n",
      "  # Determine number of features with coef > 0 along \n",
      "  # regularization path where lambda = lambda.min in CV\n",
      "  n.nonzero <- varImp(m$glmnet.fit, lambda=m$lambda.min) %>% filter(Overall > 0) %>% nrow\n",
      "  data.frame(p=p, n=n.nonzero)\n",
      "} \n",
      "r %>% ggplot(aes(x=p, y=n)) + geom_line() + theme_bw() +\n",
      "  geom_abline(intercept=0, slope=.1, linetype='dashed')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "- ESL p 651 - Using Nearest Shrunken Centroids (aka regularized diagonal linear discriminant analysis) to classify cancer samples from gene expression levels for 2308 genes and 63 samples (test set N = 20):\n",
      "    - \n",
      "    \n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Class Imbalance</center>\n",
      "\n",
      "Possible Remedies:\n",
      "- Model Tuning (optimize hyperparameters for minority class accuracy)\n",
      "- Alternate Cutoffs for prediction (see what probability prediction cutoff gives desired sensitivity)\n",
      "- Adjust prior probabilities of classes (applies to Naive Bayes and Discriminant Analysis)\n",
      "- Case Weights - This is actually similar to over/under sampling except that it is more efficient\n",
      "- Sampling Methods - Over, Under sampling or both (smote)\n",
      "- Cost Sensitive Tuning (applies to at least SVM and CART/C5.0 trees)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Discriminant Analysis</center>\n",
      "\n",
      "- LDA assumes shared covariance matrix between classes\n",
      "- QDA (quadratic discriminant analysis) allows covariance to differ\n",
      "- RDA (regularized discriminant analysis) is between the two above since it pools covariance between classes\n",
      "    - This was proposed by Friedman in 1989 and is different than shrunken centroids\n",
      "- SCRDA (shrunken centroid regularized discriminant analysis)\n",
      "- Diagonlized LDA assumes that the covariance between features within each class is 0; this is an important assumption in genomic problems as it greatly speeds computation and often improves accuracy in practice (much like Naive Bayes, which it is directly related to as a special case)\n",
      "    - This is different from shrunken centroids methods in that it only assumes independence of covariates and pooled variances, but not pooled/shrunken means in each class\n",
      "    - Tibsharini's PAM (aka \"Nearest Shrunken Centroids\") does both, use a diagonal covariance in classes and shrink centroids\n",
      "- Note that PAM (predictive analysis for microarrays is not a kind of discriminant analysis)\n",
      "- In this paper by Tibsharini and Hastie it is indicated that SCRDA is better than PAM: https://web.stanford.edu/~hastie/Papers/RDA-6.pdf"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Partial Dependence</center>\n",
      "\n",
      "Useful link explaining differences in R: http://www.r-bloggers.com/beyond-beta-relationships-between-partialplot-icebox-and-predcomps/"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Applied Machine Learning Notes</center>\n",
      "    \n",
      "<center>New Ideas</center>\n",
      "- Shrunken Centroid Models\n",
      "- Cohens Kappa for classifier performance when classes are imbalanced\n",
      "- Spatial Sign transformation\n",
      "- Partial least squares methods\n",
      "- Pearson Residuals"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}