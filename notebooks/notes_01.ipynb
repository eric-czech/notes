{
 "metadata": {
  "name": "",
  "signature": "sha256:c0c79306c5783240f03fc490fe5b4624cd58b6dc73d85934005e7572725696ef"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Reading\n",
      "- [DataTau](http://www.datatau.com/)\n",
      "- [MetaAcademy](http://www.metacademy.org/)\n",
      "- [R Tip A Day](http://onertipaday.blogspot.com/)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Statistics\n",
      "\n",
      "Cheatsheets: \n",
      "- Basics - http://www4.ncsu.edu/~swu6/documents/A-probability-and-statistics-cheatsheet.pdf"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>History</center>\n",
      "- __Geocentricity__ -- Created by Ptolemy in 2nd Century A.D. it was a model of planetary motion based on orbits of orbits that stood untested, and was very accurrate, for over 1,300 years (a great example of a bad model fitting well).\n",
      "- __5% Rule__ --  Ronald Fisher's classic textbook, Statistical Methods for Research Workers, first published in 1925: \"it is convenient to take this point as a limit in judging whether a deviation is to be considered significant or not.\"\n",
      "- __Edwin Thompson Jaynes__ -- Recommended by Richard McElreath -- [Probability Theory](http://www.cambridge.org/gb/academic/subjects/physics/theoretical-physics-and-mathematical-physics/probability-theory-logic-science) (emailed to eczech)\n",
      "- __Intuition for Gaussian__ - Given only mean and variance, gaussian is maximum entropy distribution"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Power Laws</center>\n",
      "\n",
      "A power law is a functional relationship between two quantities, where one quantity varies as a power of another (<a href=\"http://en.wikipedia.org/wiki/Power_law\">wiki</a>).\n",
      "\n",
      "####Scale Invariance\n",
      "Given a relation $ f(x) = a \\cdot x^k $, scaling the argument $x$ by a constant factor $c$ causes only a proportionate scaling of the function itself.\n",
      "\n",
      "$$ f(c x) = a(c x)^k = c^k f(x) \\propto f(x).\\!$$\n",
      "\n",
      "Scale invariance occurs iff the distribution has power \"power law tails\", i.e., tails that match the Pareto distribution up to a multiplicative constant ([source](http://rigorandrelevance.wordpress.com/2014/05/12/scale-invariance-power-laws-and-regular-variation-part-i/)).  This means \"identifying scale invariance\" is another way to identify Pareto/Power Law distributions, beyond log-log plots.\n",
      "\n",
      "####Generative Model\n",
      "\n",
      "1. Assume variable $X_t$ starts at $t = 0$\n",
      "    - Actual $X$ values at start don't seem to matter in simulations (can be random or constant)\n",
      "2. Each subsequent value is expressed as $X_t = F_t X_{t-1}$ (an example for $F$ might be that $F = 1/3$ with probability $2/3$ and $F = 1/4$ with probability $1/3$)\n",
      "    - The $F$ values can be above 1 as well, i.e. 6 with $Pr(1/3)$ and 2 with $Pr(2/3)$ (or one of them can be a fraction and the other > 1)\n",
      "3. $F_t$ is a random variable that does not depend on $X$ in any way (they're independent)\n",
      "\n",
      "This model will result in $X$ being distributed lognormally (Gilbrat first called this the \"Law of proportionate effect\").\n",
      "\n",
      "*However*, if a floor is placed on the value of $X$ such that it cannot become arbitrarily close to 0, but must rather stay above some minimum value, then $X$ will fall in a power law distribution ([source](http://www.eecs.harvard.edu/~michaelm/postscripts/im2004a.pdf)).\n",
      "\n",
      "\n",
      "\n",
      "####Known Distributions\n",
      "- Whether or not income is Pareto distributed is contentious.  Generally, lower incomes fit a lognormal curve while higher ones fit a power law.  A Double Pareto distribution may actually be closer to a correct answer ([source](http://www.eecs.harvard.edu/~michaelm/postscripts/im2004a.pdf))"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Generative process simulations\n",
      "n = 1000 # Number of entities\n",
      "m = 40   # Iterations per entity\n",
      "d = pd.DataFrame(np.random.rand(n, m))\n",
      "d = d.applymap(lambda x: 8. if x > 1/3. else 4.) # F and X\n",
      "s = np.random.random(size=n)\n",
      "\n",
      "# Run the generator process\n",
      "for i in range(n):\n",
      "    x = s[i]\n",
      "    for j in range(m):\n",
      "        x = x * d.iloc[i, j]\n",
      "    s[i] = x\n",
      "ax = pd.Series(np.log(s)).hist()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAXgAAAEACAYAAAC57G0KAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nbEp5wJuZTan/B94TZKg6q+RWAAAAAElFTkSuQmCC\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10b521cd0>"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Lognormal Distribution</center>\n",
      "\n",
      "- The ratio of two lognormal variates is also lognormal with $\\mu_Z = \\mu_X - \\mu_Y$ and $\\sigma_Z^2 = \\sigma_X^2 + \\sigma_Y^2 - 2\\sigma_{XY}$ (assuming X/Y = Z)\n",
      "- The sum or difference of two lognormal variates is neither lognormal nor normal (it's not yet well known)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Z vs T Tests</center>\n",
      "\n",
      "Calculation for both is the same, and is based on a test of the sampling distribution of the mean.  Ideally, this would be equal to:\n",
      "$$  Stat = \\frac{\\overline{x} - \\mu_0}{\\sigma_{\\overline{x}}} $$\n",
      "But $\\sigma_{\\overline{x}}$ is rarely known and must be estimated as\n",
      "$\\sigma_{x} / \\sqrt{n}$ (by the <a href=\"http://en.wikipedia.org/wiki/Central_limit_theorem\">CLT</a>).  Similarly, $\\sigma_x$ is rarely known and must be estimated as $s_{x}^2 = \\frac{1}{n-1} \\sum_{i=1}^n (y_i - \\overline{y})^2$.  Therefore the statistic is usually calculated as:\n",
      "$$  Stat = \\frac{\\overline{x} - \\mu_0}{s_x/\\sqrt{n}} $$\n",
      "And the result is usually treated as quantile for $N(0, 1)$ variate (i.e. a Z-score) if $n >= 30$, and as a t score otherwise."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Taylor Series</center>\n",
      "\n",
      "The Taylor series of a real or complex-valued function $f(x)$ that is infinitely differentiable at a real or complex number $a$ is the power series:\n",
      "\n",
      "$$ \\sum_{n=0} ^ {\\infty} \\frac {f^{(n)}(a)}{n!} \\, (x-a)^{n} $$\n",
      "\n",
      "where n! denotes the factorial of n and \u0192 (n)(a) denotes the nth derivative of \u0192 evaluated at the point a. The derivative of order zero of \u0192 is defined to be \u0192 itself and (x \u2212 a)0 and 0! are both defined to be 1. When a = 0, the series is also called a Maclaurin series.\n",
      " \n",
      "Examples:\n",
      "\n",
      "1. $ \\frac{1}{1-x} = \\sum^\\infty_{n=0} x^n\\quad\\text{ for }|x| < 1\\! $\n",
      "2. $e^x = \\sum^\\infty_{n=0} {x^n\\over n!} =1 + x + {x^2 \\over 2!} + {x^3 \\over 3!} + {x^4 \\over 4!}+\\cdots\\!, -\\infty<x<+\\infty$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Moments and Generating Functions</center>\n",
      "\n",
      "- Can be used to determine equality of distributions\n",
      "- Given a moment about the origin $u_k$ where $X$ is a random variate and $u_k = E(X^k)$, if two distributions have all $u_k$ being equal and each moment is finite, then the distributions are equal under some conditions (based on the growth rate of the moments)\n",
      "   - This is true for at least normal, poisson, and exponential \n",
      "   - Not true for lognormal\n",
      "- The __moment generating function__ (mgf) is typically more useful for the above since if the values of one distribution's mgf equal the values of another, they MUST be the same distribution, assuming the mgf is finite.  The mgf is defined as (where $X$ is a random variate):\n",
      "$$ m(t) = E(e^{tX}) = 1 + t u_1 + \\frac{u_2 t^2}{2!} + \\frac{u_3 t^3}{3!} + ... $$\n",
      "- The MGF's $k^{th}$ derivate evaulated at $t = 0$ is equivalent to the $k^{th}$ moment.   \n",
      "\n",
      "- The [method of moments](http://en.wikipedia.org/wiki/Method_of_moments_%28statistics%29) is one way to use moments to estimate the parameters of a distribution with known form.\n",
      "    - Works by stating that the sample moments calculated as $m_{k_s} = \\frac{1}{n} \\sum_{i=1}^nX_i^k $ are equal to the known expressions of the same moments in terms of the parameters.  \n",
      "    - For example in the normal distribution, 1st moment = $u$ and 2nd moment = $\\sigma^2 - u^2$ so this linear system defines estimators for the parameters $u$ and $\\sigma$ given sample data:\n",
      "        $$ m_1 = \\frac{1}{n} \\sum_{i=1}^nX_i^1 = u $$\n",
      "        $$ m_2 = \\frac{1}{n} \\sum_{i=1}^nX_i^2 = \\sigma^2 + u^2 \\implies \\sigma^2 = \\frac{1}{n} \\sum_{i=1}^n X_i^2 - u^2 = \\frac{1}{n} \\sum_{i=1}^n (X_i - \\overline{X})^2 \\text{     where }\\overline{X}\\text{ is estimator for }u$$\n",
      "    - This works as long as there is a known way to express the moments in terms of the parameters\n",
      "    \n",
      "- If $Y_1, Y_2, ... , Y_n$ are independent random variables with MGFs $m_{Y_1}(t), m_{Y_2}(t), ... , m_{Y_n}(t)$, then if $U = \\sum_{i=1}^n Y_i$, then $m_U(t) = m_{Y_1}(t) \\cdot m_{Y_2}(t) \\cdot ... \\cdot m_{Y_n}(t)$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Hypothesis Testing with Proportions</center>\n",
      "\n",
      "- Let $p$ denote the proportion of individuals or objects in a population who are in class 1 of 2\n",
      "- Let $\\hat{p}$ be the estimator for $p$ = $\\frac{X}{n}$, where $X$ is binomial variate with p = $p$\n",
      "- __Normal Approximation to Binomial__: If $np > 5$ and $n(1-p) > 5$, then: \n",
      "    - $X \\sim N(np, np(1 \u2212 p))$ \n",
      "    - $\\hat{p} \\sim \\frac{1}{n} \\cdot N(np, np(1 \u2212 p)) \\sim N(p, \\frac{p(1 \u2212 p)}{n})$  (because $Var[cX] = c^2 Var[X]$)\n",
      "    - see [wiki](http://en.wikipedia.org/wiki/Binomial_distribution#Normal_approximation) for more details\n",
      "    \n",
      "This means that a test on the proportion being equal to one found in a sample can be done using:\n",
      "\n",
      "$$Z = \\frac{\\hat{p} - p_0}{\\sqrt{p_0(1-p_0)/n}} $$\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Determining Distributions of Random Variate Functions</center>\n",
      "\n",
      "\n",
      "### With MGFs\n",
      "This can be done using Moment Generating Functions (but there are 2 other approaches as well).  Because an MGF uniquely determines a distribution, coming up with the MGF for the variate function gives a unique definition for the distribution.  For example:\n",
      "\n",
      "Given MGF = $m_X(t) = E[e^{tX}]$ where $X$ is the random variable, and given two random, standard normal variates $X \\sim N(\\mu_x, \\sigma_x^2)$ and $Y \\sim N(\\mu_y, \\sigma_y^2)$:\n",
      "\n",
      "$ m_{(X+Y)}(t) = E[e^{t(X + Y)}] = E[e^{tX}] \\cdot E[e^{tY}] $\n",
      "\n",
      "$ = e^{t\\mu_x + \\frac{1}{2}\\sigma_x^2t^2} \\cdot e^{t\\mu_y + \\frac{1}{2}\\sigma_y^2t^2} \\text{(because MGF of normal dist = } e^{t\\mu + \\frac{1}{2}\\sigma^2t^2}\\text{)} $\n",
      "\n",
      "$ = e^{ t(\\mu_x + \\mu_y)  + \\frac{1}{2}t^2(\\sigma_x^2 + \\sigma_y^2) } $\n",
      "$ \\implies X + Y \\sim N(\\mu_x + \\mu_y, \\sigma_x^2 + \\sigma_y^2) $\n",
      "\n",
      "### Using the \"Method of Distribution Functions\"\n",
      "\n",
      "Given some function of a random variable $X$, $f(X)$, where $p(x)$ is the PDF of $X$, the PDF for $f(X)$ can be determined by the following (Assuming $F_Z$ is CDF and $f_Z$ is PDF of $Z$):\n",
      "\n",
      "$$ F_{f(X)} = P(f(X) \\le Y) = P(X \\le f^{-1}(Y)) $$\n",
      "$$ = \\int_0^{f^{-1}(Y)}{f_X dx} $$\n",
      "This gives an expression for $F_{Y}$ (i.e. the CDF for $f(X)$ in terms of a new variable $Y$) which allows for $f_Y$ to be calculated as $F^{\\prime}_{Y}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>ANOVA</center>\n",
      "\n",
      "- ANCOVA\n",
      "    - Basically ANOVA with an extra covariate, always some numeric value\n",
      "    - This is helpful in an experiment where you're interested in treatment effects but have some other continuous value you want to control for, but don't particularly care about the effect of that value\n",
      "    - Assumes the same slope between covariate and response in each treatment\n",
      "        - This is verified by adding an interaction term between the covariate and treatment -- if significant, there is no homogeneity of slopes\n",
      "    - [Steps taken often](http://en.wikipedia.org/wiki/Analysis_of_covariance#Conducting_an_ANCOVA):\n",
      "        1. Determine if the covariates are correlated (> .5 spearmans) -- if so remove some\n",
      "        2. Run regression and test error variances across groups using Levene's test \n",
      "        3. Test for homogeneity of slope by running regression with CV (covariate) and IV (ind variable) interaction term -- if present, then slopes are not equal and ANCOVA should not be used\n",
      "        4. Run the regression without the interaction above which will return adjusted means, or means in treatment groups after controlling for covariates\n",
      "        "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>MCMC</center>\n",
      "\n",
      "- metropolis algorithm\n",
      "    - TODO\n",
      "- metropolis-hastings (MH) is version of metropolis that does not require symmetric proposals\n",
      "- gibbs sampling is version of MH with very clever proposals\n",
      "    - often has much faster burn-in than MH\n",
      "    - tends to get stuck (degenerates to random walk) with large number of correlated parameters\n",
      "    - this requires conjugate priors for each *parameter* -- you don't need to know the overall model posterior but you do need to be able to workout the posterior for each parameter analytically\n",
      "    - conjugacy constraint has made Gibbs fall out of fashion because conjugate priors can have \"inconvient features near boundaries in parameter space\" (taken from [Mcelreath video](https://www.youtube.com/watch?v=OJ3yaHY4th4) 35:06)\n",
      "   "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Relative Risk vs Odds Ratios</center>\n",
      "\n",
      "<table>\n",
      "<th></th><th>Experimental Group</th><th>Control Group</th>\n",
      "<tr><td>Event</td><td>a</td><td>b</td></tr>\n",
      "<tr><td>No Event</td><td>c</td><td>d</td></tr>\n",
      "</table>\n",
      "- Odds Ratio \n",
      "    - Ratio of odds = $OR = \\frac{a/b}{c/d}$\n",
      "    - $ln(OR) \\sim \\mathcal{N}(0, \\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c} + \\frac{1}{d})$\n",
      "    - 95% CI = $ln(OR) \\pm 1.96 \\sqrt{\\frac{1}{a} + \\frac{1}{b} + \\frac{1}{c} + \\frac{1}{d}}$\n",
      "    - if exponentiated bounds of CI include 1, there is no association between the groups\n",
      "- Relative Risk\n",
      "    - Ratio of occurrences over totals = $RR = \\frac{a/(a + b)}{c/(c + d)}$\n",
      "    - Same as $OR$ when occurences of disease/event are rare\n",
      "    - $ln(RR) \\sim \\mathcal{N}(0, \\sqrt{\\frac{b}{a(a+b)} + \\frac{d}{c(c+d)}})$\n",
      "    - 95% CI = $ln(RR) \\pm 1.96 \\sqrt{\\frac{b}{a(a+b)} + \\frac{d}{c(c+d)}}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Exact Tests</center>\n",
      "- [Binomail Test](http://en.wikipedia.org/wiki/Binomial_test) - Assume $p$ is some value, compute probability of data seen AND data less likely as p-value\n",
      "- [Multinomial Test](http://en.wikipedia.org/wiki/Multinomial_test) - Much like the Binomial test except that determining what data is \"as extreme\" as what was seen is less obvious.  In the multinomial test, the p-value includes the exact probability of the data seen as well as every data vector that is less likely than what was seen.  In this way, \"less likely\" is made equal to \"more extreme\" (as opposed to being based on the data directly) but this definition is more controversial in other settings.\n",
      "    - Computing the probabilities for \"all other less likely\" points than that seen can be a huge computational problem.  To remedy this, a likelihood ratio test is sometimes used involving the probability of the data seen under the null hypothesis (which may be equal $p_i$ values or anything else) and the probability of the data seen under maximum likelihood estimates for the probabilities (i.e. $p_i = x_i / N$).  Then, due to [Wilks's Theorem](http://en.wikipedia.org/wiki/Likelihood-ratio_test#Distribution:_Wilks.27s_theorem) the value $-2 ln( L_1 / L_2 )$ follows a chi-square distribution."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Gamma</center>\n",
      "\n",
      "## Function\n",
      "$$ \\Gamma (x) = \\int_0^{\\infty} t^{x-1} e^{-t} dt$$\n",
      "\n",
      "### Identities\n",
      "1. $ \\Gamma (x + 1) = x!$\n",
      "\n",
      "## Distribution PDF\n",
      "$$f(x) = \\frac{x^{k-1}e^{-\\frac{x}{\\theta}}}{\\theta^k\\Gamma(k)} \\quad \\text{ for } x, k, \\theta > 0\\text{.  k is shape, }\\theta\\text{ is scale}$$\n",
      "### Identities\n",
      "2. MGF of $ \\Gamma $ = $(1 - \\theta t)^{-n}$\n",
      "3. if $X_i \\sim N(u, \\sigma^2)$, then $\\sum_{1}^{n}{(X_i - \\bar{X})^2} \\sim \\sigma^2 \\chi^2_{n} \\sim \\Gamma(n/2, 2\\sigma^2)$\n",
      "4. Given $X \\sim \\Gamma(\\alpha, \\theta)$ and $Y \\sim \\Gamma(\\beta, \\theta)$ then $\\frac{X}{X + Y} \\sim B(\\alpha, \\beta)$\n",
      "5. Above generalizes to Dirichlet distribution for $N$ Gamma distributed i.i.d RV's"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Confidence and Credible Intervals</center>\n",
      "\n",
      "- A good [explanation of the difference](http://stats.stackexchange.com/questions/2272/whats-the-difference-between-a-confidence-interval-and-a-credible-interval)\n",
      "- __Confidence Intervals__\n",
      "    - Created by Jerzy Neyman in 1937\n",
      "    - A confidence interval does not predict that the true value of the parameter has a particular probability of being in the confidence interval given the data actually obtained (a prior is necessary for this)\n",
      "    \n",
      "    - For any kind of model (including glms) confidence intervals on parameters can be calculated one of many ways.  Each involves different tradeoffs for accuracy vs speed.\n",
      "        - See [this post](http://stats.stackexchange.com/questions/117641/how-trustworthy-are-the-confidence-intervals-for-lmer-objects-through-effects-pa) for a comparison of common methods\n",
      "        - Wald based confidence intervals are NOT the default for lmer, it uses profile likelihood intervals instead\n",
      "        - Parametric bootstrapping is the gold standard"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Order Statistics</center>\n",
      "\n",
      "- Given $n$ i.i.d. random variables with CDF $F_{X}$, the CDF of $Y = max_{i=1:n}(X_i)$ is equal to the product of the individual CDF values.  Therefore, $ F_Y(u) = F_X(u) ^ n$ and this can be differentiated to get the PDF"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Normal Distribution Approximations</center>\n",
      "\n",
      "- Binomial is roughly normal if $np > 5$ and $nq > 5$\n",
      "- Beta distribution is roughly normal if $\\alpha_1$ and $\\alpha_2$ are greater than 10, or more generally if for both parameters: $ \\frac{\\alpha + 1}{\\alpha - 1} \\approx 1 $"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Observer Bias</center>\n",
      "\n",
      "- Occurs when random sampling results in cases being oversampled in proportion to their magnitude\n",
      "- For example:\n",
      "    - trains may arrive with either a 10 min delay or a 90 min delay \n",
      "    - the number of trains with either delay may be equal (50% chance of each)\n",
      "    - an \"observer\" just catching a train will think that 90% of the trains run with a 90 min delay\n",
      "    - the real probability of each delay is equal to the observed rate divided by the delay size \n",
      "    - inveresely, the observed probability of a delay is equal to it's real probability times the delay size"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Logistic Regression</center>\n",
      "- Model involves application of __Logistic Function__:\n",
      "$$ F(x) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 x)}} $$\n",
      "where F(x) is the probability of a binary response being positive.\n",
      "- The __Logit__ is the inverse of the logistic function such that:\n",
      "$$ logit(F(x)) = ln{\\frac{F(x)}{1 - F(x)}} = \\beta_0 + \\beta_1 x$$\n",
      "- The __Logit__ is also known as log odds or generally $ln(\\frac{p}{1-p})$\n",
      "- There is no closed form solution for the MLE of \\beta_1.  Instead iterative methods like Newton's method are used.\n",
      "- Maximum likelihood estimation intuition: [Simple Logistic Regression](http://www.wright.edu/~thaddeus.tarpey/ES714glm.pdf)\n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>PCA</center>\n",
      "\n",
      "- Given principal component vectors (eigenvectors), transformed points are calculated by subtracting the \"center\" from the PCA result and then taking the dot product between any observation and a principal component vector to give the scalar result along that PC\n",
      "In R:\n",
      "```\n",
      "data = data.frame(...)\n",
      "pca = princomp(data)\n",
      "value_for_observation_1_along_PC_1 = sum( (data[1,] - pca$center) * t(pca$loadings[,1]))\n",
      "```\n",
      "- Functional PCA ( [A good conversation on this](http://stats.stackexchange.com/questions/23566/functional-principal-component-analysis-fpca-what-is-it-all-about) -- answer 2 )\n",
      "    - This is essentially a continous analog of normal PCA where discrete, noisy time series are fit to a series of smooth basis functions where are then used to give principal component _functions_ (instead of just a transformation) \n",
      "    - The results are not likely to be significantly different from smoothing the time series and doing a normal PCA __OR__ just smooth the PCA transformed time series (i.e. the \"eigen-time-series\")\n",
      "\n",
      "\n",
      "- PCA is very similar to OLS regression except that instead of minimizing the error at angles to the fit line, the error is minimized perpendicular to that line (also note that y ~ x, x ~ y, and PCA(1) are all different results)"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Cross Correlation</center>\n",
      "\n",
      "- Pearson correlation between timeseries at different lags\n",
      "- For significance, the cross correlation has mean 0 and variance $1/n$ under the null hypothesis implying the 95% CI: $0 \\pm 2/\\sqrt{n}$"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Fixed vs Random Effects</center>\n",
      "\n",
      "- Assume a dataset with test scores, student #, test #, and amount of time studied where the regression problem is determining score as a function of study time:\n",
      "    - This will likely suffer from omitted variable bias because of the fact that some students are smarter AND those same smarter students are likely to spend more time studying (the correlation is important) leading to a higher coefficient estimate\n",
      "    - A \"Fixed effects\" model would assume there will be a specific addition to the test score in the model based on who the student is, added to the model as dummy binary variables (N variables for N students)\n",
      "    - Adding so many dummy binary variables is cumbersome and can cause other issues \n",
      "    - A \"Random effects\" model would add a _single_ extra random variable (instead of many dummy variables) along with a random error term.  Note that this is only a good model if the individual effects (eg intelligence) are NOT correlated with study time which is likely not true in this case.  If that assumption is false, then ommitted variable bias is introduced.\n",
      "    - Random effects terms must be estimated with specific techniques, not OLS regression\n",
      "    - Most models are fixed effects models\n",
      "     \n",
      "- In study meta-analysis:\n",
      "    - Interpretation ->\n",
      "        - Fixed effects assume that the effect mean being studied by different researchers is truly EQUAL \n",
      "        - Random effects assume that the effect mean is truly DIFFERENT\n",
      "    - Weighting ->\n",
      "        - Fixed effects weights of individual studies set as $w_i = \\frac{1}{\\sigma_i^2/n_i}$ and used to estimate a weighted average for the overall effect as $\\frac{\\sum{w_i \\cdot T_i}}{\\sum{w_i}}$ where $T_i$ is the mean for the $i$th study\n",
      "        - Random effects weights are $w_i = \\frac{1}{\\sigma_i^2/n_i + \\tau^2}$ where $\\tau^2$ is the \"between study variance\"\n",
      "        - Random effects models always have more balanced weights across different studies (i.e. the small studies are more improtant)\n",
      "        - The standard error of the overall effect is always higher for random effects\n",
      "    - Selection -> \n",
      "        - Fixed effects are the correct choice if there is good reason to believe that the effect is equal across studies (usually only true if it's the same researchers).  Random effects is better for less similar units.\n",
      "        - There are tests for determining if the between studies variance is 0 (in which case fixed-effects is better) but the use of that test is usually discouraged and using logic like the above preferrred\n",
      "    - A good [paper on the subject](http://www.meta-analysis.com/downloads/Meta-analysis%20Fixed-effect%20vs%20Random-effects%20models.pdf)\n",
      "    \n",
      "- A comparison of similar models:\n",
      "    - The grand/pooled estimate: $ \\hat{y} = \\hat{x} \\beta$\n",
      "    - The unpooled estimate: $ \\hat{y_i} = \\hat{x_i} \\beta$\n",
      "    \n",
      "    - Fixed effects models = varying intercept models = Least Squares Dummy Variable models\n",
      "    - varying intercept AND slope models are equivalent to running separate regressions\n",
      "    - HLM (hierarchical linear modeling, which is a random effects model) produces results between the pooled and unpooled estimates\n",
      "    - HLM models produce biased beta estimates but with a lower variance; Fixed effects models are unbiased but more variable\n",
      "        - This means the former throws more false positives on beta p-tests, but also fewer false negatives\n",
      "    - Deciding whether or not to use HLM / Random Effects models\n",
      "        - If unit heterogeneity is small, HLM is close to pooled estimate\n",
      "        - If unit heterogeneity is large, HLM is close to unpooled estimate\n",
      "        - Typically, it would only be used if unit heterogeneity is low to moderate"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Machine Learning"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Best Practices</center>\n",
      "\n",
      "From Kaggle chief scientist: https://www.youtube.com/watch?v=9Zag7uhjdYo\n",
      "\n",
      "- Boruta feature selection algorithm\n",
      "- Porter stemming algorithm used in NLP problems\n",
      "- Ensembling 2 to dozens of models yields 1-5% improvement\n",
      "- Factorization machines? are possibly the only competitor to normal ML ensembles"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Imputation</center>\n",
      "\n",
      "- Multiple Imputation\n",
      "    - Use present values in regression for missing values (including dependent variable)\n",
      "    - Use the asymptotic distribution of the $\\beta$ coefficients as well as the VCV matrix ($\\beta$ is multivariate normal) to draw values of $\\beta$ to create missing values multiple times\n",
      "    - Impute missing values using all the different $\\beta$ draws\n",
      "    - Calculate the prediction for $y$ $m$ times and use an estimate for each $\\beta$ as $\\hat{\\beta} = \\frac{1}{m} \\sum{\\beta_m}$\n",
      "    - There is then also a formula for getting the standard error of the \"final\" $\\beta$ -- see [this video](https://www.youtube.com/watch?v=qfpZcsb32Z4&index=1&list=PLAFC5F02F224FA59F) (at minute 37)\n",
      "    "
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Model Generalizations</center>\n",
      "\n",
      "- [Stacked Generalization](http://www.machine-learning.martinsewell.com/ensembles/stacking/Wolpert1992.pdf) (aka Blending, Stacking)\n",
      "    - See answer [here](http://stats.stackexchange.com/questions/102631/k-fold-cross-validation-of-ensemble-learning) for CV process with this\n",
      "    - CV is nested (presumably with same k) and proceeds like this:\n",
      "        1. Train level 0 models on nested training set and save predictions on validation set\n",
      "        2. Train level 1 model on predictions from each validation set (the # of predictions will equal size of outer fold)\n",
      "        3. Retrain level 0 models on entire outer training fold\n",
      "        4. Make level 0 predictions on outer validation set and feed those in to level 1 model trained in step #2\n",
      "    - if satisfied with the performance shown above in CV, then use a NON-nested CV to create a final model as follows:\n",
      "        1. Train level 0 models on each training set\n",
      "        2. Train level 1 model on predictions from level 0\n",
      "        3. Retrain level 0 models on entire data set\n",
      "- [Ensemble Selection](http://www.cs.cornell.edu/~caruana/ctp/ct.papers/caruana.icml04.icdm06long.pdf)\n",
      "    - Works as follows:\n",
      "        1. For a given CV fold training set, train thousands of models on that set (often many of the same algorithms with different params) and create validation set predictions for them\n",
      "        2. Initialize the final ensemble as the empty set\n",
      "        3. Determine the performance of the ensemble if each other model was added to it; the ensemble performance is determined by any performance metric whatsoever and ensemble outputs are averages of member guesses)\n",
      "        4. Add to the ensemble the model that results in the best overall performance for the __ensemble__\n",
      "        5. Also, add the selected model back into the pool of candidates (if this isn't done overfitting can occur and this should result in better models being selected potentially many times)\n",
      "        6. Repeat #3-6 until no more gain can be acheived\n",
      "    - Note that there are a few other suggested improvements like not starting with an empty set (using the top N models instead) and randomly choosing a sample of the candidate models from the outset, repeating the whole procedure many times, and creating a final ensemble that is an average of all of those ensembles\n",
      "    - This approach is nice because it deals with hyperparameter optimization and can optimize for many metrics at once"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Recommendation Algorithms</center>\n",
      "\n",
      "- __Supervised Recommendation Problems__\n",
      "    - Per-User Model to recommend news content (approach 1 -- taken [from this paper](http://www.cs.cmu.edu/~anatoleg/gershman-wolfe-fink-carbonell.pdf))\n",
      "        - Extract features from each document the user has viewed including tf-idf, popularity + age of article, as well as named entities\n",
      "        - Positive responses are cases where document was browsed or read in full; Negative cases were those skimmed through or opened very briefly \n",
      "        - SVM was used to predict the positive or negative reaction of a user to a new article\n",
      "    - Per-User Model (approach 2)\n",
      "        - Input features is up to IxU where U = number of users - 1, I = number of items rated by active user (i.e. the one being modeled) previously\n",
      "        - Response is rating of active user for that product"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "#Physics"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Particles</center>\n",
      "\n",
      "- Wolfgang Pauli postulated the neutrino in 1930 which was confirmed experimentally in 1956, 2 years before his death (this confirmed a loss of mass during \"Beta decay\" where neutron becomes proton + electron and the missing mass was the neutrino)\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Quantum Physics</center>\n",
      "\n",
      "- Qubit - 2 state quantum-mechanical system\n",
      "- Examples of quantum-mechanical systems that can be binarized:\n",
      "    - Photon polarization\n",
      "    - Electron spin\n",
      "    - Nuclear spin\n",
      "    - Electric current direction in superconductors\n",
      "- Quantum-mechanical systems with 2 states all have the same underlying properties, e.g. w/ photon polarization:\n",
      "    - Oscillation of waves is always perpendicular to direction of travel but can be at any angle otherwise (think coming towards you)\n",
      "    - Assuming there is a $\\pm0^{\\circ}$ and $\\pm90^{\\circ}$ direction, a photon found to be $+90^{\\circ}$ will remain $+90^{\\circ}$ until measured at some other angle\n",
      "    - If a $+45^{\\circ}$ photon is measured for $\\pm0^{\\circ}$ / $\\pm90^{\\circ}$ polarization, it will become one of those randomly (probability of 50%) but will remain in whatever state was chosen\n",
      "    - That same photon will no longer be $+45^{\\circ}$ -- if measured for $\\pm45^{\\circ}$ again, it will emerge with + or - angle with equal probability\n",
      "    - A photon with $+10^{\\circ}$ polarization will be more likely to be $\\pm0^{\\circ}$ polarized than $\\pm90^{\\circ}$, if measured for both simultaneously\n",
      "        - This state _superposition_ is usually described as: \n",
      "            $$| \\psi \\rangle = \\alpha |0 \\rangle + \\beta |1 \\rangle$$\n",
      "          where $$| \\alpha |^2 + | \\beta |^2 = 1$$\n",
      "          and the probability of outcome $|0 \\rangle$  is $| \\alpha |^2$ and the probability of outcome $|1 \\rangle$  is $| \\beta |^2$\n",
      "        - If a _Standard Basis Measurement_ is made, the state will be known as $|0 \\rangle$ or $|1 \\rangle$, and $\\alpha$ or $\\beta$ will be 1 while the other becomes 0\n",
      "    - Before quantum physics, it was assumed that causal laws of nature like Newtonian mechanics determined everything but it seems intdeterminism is a fundamental part of nature\n",
      "    \n",
      "### Polarized Lenses\n",
      "\n",
      "Work by forcing photon polarization to collapse to perpendicular or horizontal state, lettting vertically polarized photons through and absorbing the others.  The horizontally polarized photons are removed because they are more common when reflected from horizontal surfaces like water.  If you tilt polarized sunglasses 90 degrees, they no longer block out glare from horizontal surfaces.\n",
      "    \n",
      "### Cryptographic Key Exchange\n",
      "\n",
      "- Alice wants to share a symmetric key with Bob\n",
      "- Assume each has Stern-Gerlach magnets that can detect +/- (H)orizontal/(V)ertical electron spin (4 possibilities)\n",
      "- Alice will send a sequence of electrons and will decide to meausure the +/- direction of each, choosing randomly to do so at the H or V orientation\n",
      "- Bob will receive electrons and also decide randomly whether or not to measure at H or V orientation\n",
      "- When done, Alice and Bob publicly share the orientations they chose but not the measurement results\n",
      "- They exclude results where they did not randomly choose the same orientation and the + or - results from the remainder become the key\n",
      "\n",
      "Note, an eavesdropper that knows the the magnet orientations used by both can intercept the message without destroying it.  Randomly choosing H or V before transmission means that even if an eavesdropper sees the orientation at the end, she won't have known it at the time of transmission and couldn't have possibly guessed the same orientations.  This person could however decode the key if he/she could get a hold of the electrons again after the exchange was complete.\n",
      "\n",
      "### Hidden Variables\n",
      "\n",
      "These would include some explanation for randomness observed in quantum system due to unobservable entities in 3 dimensional space.  However, John Bell proved in 1964, via [Bell's Theorem](http://en.wikipedia.org/wiki/Bell%27s_theorem), that unseen __local__ variables cannot exist if predictions of quantum mechanices are true.  This was derived from quantum entaglement and indicates that there isn't some yet unknown classical explanation of quantum randomness."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}
