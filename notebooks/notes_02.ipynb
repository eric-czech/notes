{
 "metadata": {
  "name": "",
  "signature": "sha256:ddeb0aaeb8230316398b0d76a7a4e897811fb2c55e03b077f4bbc1eb4d7cd930"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Kullback-Liebler Divergence</center>\n",
      "\n",
      "- non-symmetric measure of difference between two distributions\n",
      "- created in 1951\n",
      "\n",
      "\n",
      "- definition: $$ D_{KL}(P || Q) = \\sum_{i}{P(i)ln\\frac{P(i)}{Q(i)}} $$\n",
      "- if $P$ is the true model above, then for the sake of modeling $P$ is never truly known.  Instead [deviance](http://en.wikipedia.org/wiki/Deviance_%28statistics%29) is used which is really defined as the difference in KL divergence between two competing models $Q$ and $R$.  In that case, the values associated with the true distribution $P$ \"subtract out\" leaving just the definition of deviance, or relative KL divergence as :\n",
      "    $$ D = ? $$ (need to watch the elreath video to see why the subtraction really works)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}