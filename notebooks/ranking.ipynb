{
 "metadata": {
  "name": "",
  "signature": "sha256:42aebf744287ad8a4a431e0476acd89107fe7bd2ea07522368eea8a2b0e2c97b"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "* * * \n",
      "##<center>Ranking Approaches</center>\n",
      "\n",
      "- [Learning to Rank](https://en.wikipedia.org/wiki/Learning_to_rank)\n",
      "- 3 different kinds of losses: Pointwise, Pairwise, and Listwise\n",
      "- Pairwise seem to be the most common\n",
      "- There is a way to convert ANY ranking problem into a classification problem:\n",
      "    - The idea is to first have \"groups\" in your data (this may be people or dates) and then for each group, create a new dataset with all pairs of original data points where the features become the difference between the feature vectors for the pair, and the classification target is the sign of the differences in ranks\n",
      "    - See [here](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=1&ved=0ahUKEwjEl6ekvLPMAhUFOSYKHX--CbEQFggiMAA&url=http%3A%2F%2Fwww.herbrich.me%2Fpapers%2Ficann99_ordinal.pdf&usg=AFQjCNEVz5TkgmyWk-neGxFDbsUUfEJ4NA&sig2=O9SGc-qHueXIWoKfa-ubvg) for a paper on why the above is true (the paper is called \"Support Vector Learning for Ordinal Regression by Herbrich\")\n",
      "    - See [here](http://fa.bianp.net/blog/2012/learning-to-rank-with-scikit-learn-the-pairwise-transform/) for a python implementation of this\n",
      "- In Kaggle competitions some people say that using XGBoost pairwise rank loss does no better than a regression with least squared loss\n",
      "\n",
      "### Ranking Loss Functions\n",
      "\n",
      "- sklearn has a [ranking loss function](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.label_ranking_loss.html#sklearn.metrics.label_ranking_loss)\n",
      "- The above is defined in [this paper](http://lpis.csd.auth.gr/publications/tsoumakas09-dmkdh.pdf) (page 14)\n",
      "- XGBoost has a \"pairwise\" rank loss\n",
      "    - Details on setting \"groups\" for XGBoost: [SO Post](http://datascience.stackexchange.com/questions/10179/how-fit-pairwise-ranking-models-in-xgboost)\n",
      "    - The [__LambdaRank__](http://research.microsoft.com/pubs/68133/lambdarank.pdf) loss function is what XGBoost uses (the link is to the original paper on it)\n",
      "    - The key insight behind LambdaRank is that it does not work directly with loss measures based on pairs of data points -- according to the paper \"The key point is that although the overall computation still has an n^2 dependence arising from the second sum in (7), computing the terms (some part of gradient) is far cheaper than the computation required to perform the 2n fprops and n backprops. Thus we have effectively replaced an O(n^2) one with a O(n) one\"\n",
      "    - The LambdaRank loss has a physical analogy in minimizing potential energy based on the scores associated with data points (i.e. predictions)\n",
      "    \n",
      "### Ranking Performance Measures\n",
      "\n",
      "Common Measures:\n",
      "\n",
      "- NGDC: Normalized Discounted Cumulative Gain \n",
      "- MAP: Mean Average Precision"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}